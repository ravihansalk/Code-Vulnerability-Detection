{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "file_path = 'clasification_Training.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "functions = df['input'].tolist()\n",
    "\n",
    "labels = df['output'].str.lower().apply(lambda x: 1 if x == 'vulnerable' else 0)\n",
    "print(len(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_code(code):\n",
    "    code = re.sub(r'//.*|/\\*[\\s\\S]*?\\*/', '', code)\n",
    "    code = re.sub(r'\".*?\"|\\'.*?\\'', '', code)\n",
    "    tokens = word_tokenize(code)\n",
    "    return tokens\n",
    "\n",
    "preprocessed_functions = [preprocess_code(func) for func in functions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(sentences=preprocessed_functions, vector_size=100, window=5, min_count=1, workers=4)\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokens_to_embedding(tokens, model, max_len):\n",
    "    embedding = np.zeros((max_len, model.vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i >= max_len:\n",
    "            break\n",
    "        if token in model.wv:\n",
    "            embedding[i] = model.wv[token]\n",
    "    return embedding\n",
    "\n",
    "max_len = 50\n",
    "X = np.array([tokens_to_embedding(tokens, w2v_model, max_len) for tokens in preprocessed_functions])\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(hp.Int('units1', min_value=32, max_value=256, step=32),\n",
    "                   input_shape=(max_len, w2v_model.vector_size), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(LSTM(hp.Int('units2', min_value=32, max_value=128, step=32), return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(LSTM(hp.Int('units3', min_value=32, max_value=64, step=16)))\n",
    "    model.add(Dropout(hp.Float('dropout3', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "                  learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  \n",
    "    executions_per_trial=2,  \n",
    "    directory='hyperparameter_tuning',\n",
    "    project_name='lstm_tuning')\n",
    "\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=20, validation_split=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hps.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "epochs = range(1, 21)  \n",
    "plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(epochs) \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()\n",
    "\n",
    "\n",
    "trials = tuner.oracle.get_best_trials(num_trials=10)\n",
    "trial_scores = [trial.score for trial in trials]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(trial_scores) + 1), trial_scores)\n",
    "plt.title('Best Trial Scores')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred_binary = np.round(y_pred).astype(int)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
