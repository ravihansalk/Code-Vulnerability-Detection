{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import ast\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class ASTVisitor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        self.tokens.append(f\"FunctionDef:{node.name}\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Name(self, node):\n",
    "        self.tokens.append(f\"Name:{node.id}\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Str(self, node):\n",
    "        self.tokens.append(f\"Str:{node.s}\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_Constant(self, node):\n",
    "        # For Python 3.8+\n",
    "        if isinstance(node.value, str):\n",
    "            self.tokens.append(f\"Str:{node.value}\")\n",
    "        self.generic_visit(node)\n",
    "\n",
    "# Connect to the database and fetch data\n",
    " \n",
    "\n",
    "server = 'SQLServer'\n",
    "database = 'CodeStorageDB'\n",
    "connection_string = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = pyodbc.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "    SELECT  [FunctionContent], [vulnerability]\n",
    "    FROM [dbo].[CodeFunctionsAST] \n",
    "''')\n",
    "\n",
    "functions = []\n",
    "labels = []\n",
    "\n",
    "# Fetch the data and create the lists\n",
    "for row in cursor:\n",
    "    # Parse the code into AST\n",
    "    parsed_code = ast.parse(row.FunctionContent)\n",
    "    \n",
    "    # Extract tokens from AST\n",
    "    visitor = ASTVisitor()\n",
    "    visitor.visit(parsed_code)\n",
    "    tokens = visitor.tokens\n",
    "    #print(tokens)\n",
    "    # Append tokens and labels to lists\n",
    "    functions.append(tokens)\n",
    "    labels.append(row.vulnerability)\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Preprocess tokens (if necessary, adjust according to your tokenization needs)\n",
    "def preprocess_tokens(tokens):\n",
    "    # This function can be modified if additional preprocessing is needed\n",
    "    return tokens\n",
    "\n",
    "preprocessed_functions = [preprocess_tokens(tokens) for tokens in functions]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=preprocessed_functions, vector_size=100, window=5, min_count=1, workers=4)\n",
    "w2v_model.save(\"word2vec.model\")\n",
    "\n",
    "# Convert tokens to embeddings\n",
    "def tokens_to_embedding(tokens, model, max_len):\n",
    "    embedding = np.zeros((max_len, model.vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i >= max_len:\n",
    "            break\n",
    "        if token in model.wv:\n",
    "            embedding[i] = model.wv[token]\n",
    "    return embedding\n",
    "\n",
    "max_len = 50\n",
    "#print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([tokens_to_embedding(tokens, w2v_model, max_len) for tokens in preprocessed_functions])\n",
    "y = np.array(labels)\n",
    "print(X)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_len, w2v_model.vector_size), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([tokens_to_embedding(tokens, w2v_model, max_len) for tokens in preprocessed_functions])\n",
    "y = np.array(labels)\n",
    "\n",
    "#print(X)  # Optional: Print the shape of X to verify\n",
    "print(len(X))\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Flatten the embeddings for the DNN input\n",
    "X_flattened = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flattened, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(X_flattened.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Example hyperparameter values\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.5\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
    "\n",
    "model = create_model(learning_rate=0.001, dropout_rate=0.5)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, model.predict(X_test_scaled) > 0.5)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, model.predict(X_test_scaled))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, model.predict(X_test_scaled))\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
